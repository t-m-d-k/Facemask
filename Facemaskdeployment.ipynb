{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the relevant libraries\n",
    "import numpy as np\n",
    "import cv2 # openCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5.2\n"
     ]
    }
   ],
   "source": [
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = cv2.imread('test_gko_rs.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalefactor = 1.0/255.0\n",
    "new_size = (416, 416)\n",
    "blob = cv2.dnn.blobFromImage(test_img, scalefactor, new_size, swapRB=True, crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels_path = \"face_mask_classes.names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no face mask', 'face mask']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels = open(class_labels_path).read().strip().split(\"\\n\")\n",
    "class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_colors = [\"255,0,0\",\"0,255,0\"]\n",
    " \n",
    "# 2nd: split the array on comma-separated strings and for change each string type to integer\n",
    "class_colors = [np.array(every_color.split(\",\")).astype(\"int\") for every_color in class_colors]\n",
    " \n",
    "# 3d: convert the array or arrays to a numpy array\n",
    "class_colors = np.array(class_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = cv2.dnn.readNetFromDarknet('yolov4.cfg','yolov4_best.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layers = yolo_model.getLayerNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sets of detections: 3\n"
     ]
    }
   ],
   "source": [
    "# Extract the output layers\n",
    "output_layers = [model_layers[model_layer[0] - 1] for model_layer in yolo_model.getUnconnectedOutLayers()]\n",
    "\n",
    "# input pre-processed blob into the model\n",
    "yolo_model.setInput(blob)\n",
    " \n",
    "# compute the forward pass for the input, storing the results per output layer in a list\n",
    "obj_detections_in_layers = yolo_model.forward(output_layers)\n",
    " \n",
    "# verify the number of sets of detections\n",
    "print(\"number of sets of detections: \" + str(len(obj_detections_in_layers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection_analysis_with_nms(test_img, class_labels, class_colors, obj_detections_in_layers, score_threshold, nms_threshold):\n",
    "\n",
    "\t# get the image dimensions  \n",
    "\timg_height = test_img.shape[0]\n",
    "\timg_width = test_img.shape[1]\n",
    "\n",
    "\tresult = test_img.copy()\n",
    "\n",
    "\t# declare lists for the arguments of interest: classID, bbox info, detection confidences\n",
    "\tclass_ids_list = []\n",
    "\tboxes_list = []\n",
    "\tconfidences_list = []\n",
    "\t# loop over each output layer \n",
    "\tfor object_detections_in_single_layer in obj_detections_in_layers:\n",
    "\t\t# loop over the detections in each layer\n",
    "\t\tfor object_detection in object_detections_in_single_layer:\n",
    "\t\t\t# get the confidence scores of all objects detected with the bounding box\n",
    "\t\t\tprediction_scores = object_detection[5:]\n",
    "\t\t\t# consider the highest score being associated with the winning class\n",
    "\t\t\t# get the class ID from the index of the highest score\n",
    "\t\t\tpredicted_class_id = np.argmax(prediction_scores)\n",
    "\t\t\t# get the prediction confidence\n",
    "\t\t\tprediction_confidence = prediction_scores[predicted_class_id]\n",
    "\t\t\t#print(predicted_class_id,\"*****************\",class_labels)\n",
    "\t\t\t# consider object detections with confidence score higher than threshold\n",
    "\t\t\tif prediction_confidence > score_threshold:\n",
    "\t\t\t\t# get the predicted label\n",
    "\t\t\t\tpredicted_class_label = class_labels[predicted_class_id]\n",
    "\t\t\t\t# compute the bounding box cooridnates scaled for the input image\n",
    "\t\t\t\tbounding_box = object_detection[0:4] * np.array([img_width, img_height, img_width, img_height])\n",
    "\t\t\t\t(box_center_x_pt, box_center_y_pt, box_width, box_height) = bounding_box.astype(\"int\")\n",
    "\t\t\t\tstart_x_pt = max(0, int(box_center_x_pt - (box_width / 2)))\n",
    "\t\t\t\tstart_y_pt = max(0, int(box_center_y_pt - (box_height / 2)))\n",
    "\n",
    "\t\t\t\t# update the 3 lists for nms processing\n",
    "\t\t\t\t# - confidence is needed as a float \n",
    "\t\t\t\t# - the bbox info has the openCV Rect format\n",
    "\t\t\t\tclass_ids_list.append(predicted_class_id)\n",
    "\t\t\t\tconfidences_list.append(float(prediction_confidence))\n",
    "\t\t\t\tboxes_list.append([int(start_x_pt), int(start_y_pt), int(box_width), int(box_height)])\n",
    "\n",
    "\t# NMS for a set of overlapping bboxes returns the ID of the one with highest \n",
    "\t# confidence score while suppressing all others (non maxima)\n",
    "\t# - score_threshold: a threshold used to filter boxes by score \n",
    "\t# - nms_threshold: a threshold used in non maximum suppression. \n",
    "\n",
    "\twinner_ids = cv2.dnn.NMSBoxes(boxes_list, confidences_list, score_threshold, nms_threshold)\n",
    "\n",
    "\t# create a list of winner boxes\n",
    "\twinner_box_list = []\n",
    "\n",
    "\tfor winner_id in winner_ids:\n",
    "\t\tmax_class_id = winner_id[0]\n",
    "\t\tbox = boxes_list[max_class_id]\n",
    "\t\tstart_x_pt = box[0]\n",
    "\t\tstart_y_pt = box[1]\n",
    "\t\tbox_width = box[2]\n",
    "\t\tbox_height = box[3]\n",
    "\t\twinner_box_list.append(box)\n",
    "\n",
    "\t\t#get the predicted class id and label\n",
    "\t\tpredicted_class_id = class_ids_list[max_class_id]\n",
    "\t\tpredicted_class_label = class_labels[predicted_class_id]\n",
    "\t\tprediction_confidence = confidences_list[max_class_id]\n",
    "\n",
    "\t\t#obtain the bounding box end co-oridnates\n",
    "\t\tend_x_pt = start_x_pt + box_width\n",
    "\t\tend_y_pt = start_y_pt + box_height\n",
    "\n",
    "\t\t#get a random mask color from the numpy array of colors\n",
    "\t\tbox_color = class_colors[predicted_class_id]\n",
    "\n",
    "\t\t#convert the color numpy array as a list and apply to text and box\n",
    "\t\tbox_color = [int(c) for c in box_color]\n",
    "\n",
    "\t\t# print the prediction in console\n",
    "\t\tpredicted_class_label = \"{}: {:.2f}%\".format(predicted_class_label, prediction_confidence * 100)\n",
    "\t\tprint(\"predicted object {}\".format(predicted_class_label))\n",
    "\n",
    "\t\t# draw rectangle and text in the image\n",
    "\t\tcv2.rectangle(result, (start_x_pt, start_y_pt), (end_x_pt, end_y_pt), box_color, 1)\n",
    "\t\tcv2.putText(result, predicted_class_label, (start_x_pt, start_y_pt-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 1)\n",
    "\n",
    "\treturn result, winner_box_list, predicted_class_label\n",
    "\n",
    "\n",
    "def object_detection_iou(iou_image, detection_box, predicted_class_label, gt_box=None):\n",
    "\tstart_pt_x_box_a = detection_box[0]\n",
    "\tstart_pt_y_box_a = detection_box[1]\n",
    "\tend_pt_x_box_a = detection_box[0] + detection_box[2]\n",
    "\tend_pt_y_box_a = detection_box[1] + detection_box[3]\n",
    "\tcv2.rectangle(iou_image, (start_pt_x_box_a, start_pt_y_box_a), (end_pt_x_box_a, end_pt_y_box_a), (0, 255, 0), 2)\n",
    "\tcv2.putText(iou_image, predicted_class_label, (start_pt_x_box_a, start_pt_y_box_a-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "\t# start_pt_x_box_b = gt_box[0]\n",
    "\t# start_pt_y_box_b = gt_box[1]\n",
    "\t# end_pt_x_box_b = gt_box[0] + gt_box[2]\n",
    "\t# end_pt_y_box_b = gt_box[1] + gt_box[3]\n",
    "\t# cv2.rectangle(iou_image, (start_pt_x_box_b, start_pt_y_box_b), (end_pt_x_box_b, end_pt_y_box_b), (0, 0, 255), 2)\n",
    "\t# cv2.putText(iou_image, \"ground truth bbox\", (start_pt_x_box_b, start_pt_y_box_b-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "\n",
    "\t# determine the (x, y)-coordinates of the intersection rectangle\n",
    "\t# xA = max(start_pt_x_box_a, start_pt_x_box_b)\n",
    "\t# yA = max(start_pt_y_box_a, start_pt_y_box_b)\n",
    "\t# xB = min(end_pt_x_box_a, end_pt_x_box_b)\n",
    "\t# yB = min(end_pt_y_box_a, end_pt_y_box_b)\n",
    "\n",
    "\t# # compute the area of intersection rectangle\n",
    "\t# intersection_area = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\n",
    "\t# # compute the areas of both rectangles  separately\n",
    "\t# detArea = (end_pt_x_box_a - start_pt_x_box_a + 1) * (end_pt_y_box_a - start_pt_y_box_a + 1)\n",
    "\t# gtArea = (end_pt_x_box_b - start_pt_x_box_b + 1) * (end_pt_y_box_b - start_pt_y_box_b + 1)\n",
    "\t# unionArea = detArea + gtArea - intersection_area\n",
    "\n",
    "\t# # compute the intersection over union \n",
    "\t# iou_value = intersection_area / float(unionArea)\n",
    "\t# cv2.putText(iou_image, \"IoU: {:.4f}\".format(iou_value), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\t# print(\"iou = {:.4f}\".format(iou_value))\n",
    "\n",
    "\t# return the intersection over union value\n",
    "\treturn iou_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score_threshold = 0.5\n",
    "nms_threshold = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted object face mask: 99.32%\n"
     ]
    }
   ],
   "source": [
    "result, winner_boxes, predicted_class_label = object_detection_analysis_with_nms(test_img, class_labels, class_colors, obj_detections_in_layers, score_threshold, nms_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from object_detection_functions import object_detection_iou\n",
    " \n",
    "# create a copy of the test image\n",
    "iou_image = test_img.copy()\n",
    " \n",
    "# print the ground truth and detection bounding boxes, and the IoU value\n",
    "iou_image = object_detection_iou(test_img, winner_boxes[0],predicted_class_label)\n",
    "\n",
    "#cv2.imshow('demo',iou_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = cv2.waitKey(0)\n",
    "if k == 27:         # wait for ESC key to exit\n",
    "    cv2.destroyALLWindows()\n",
    "elif k == ord('s'): # wait for 's' key to save and exit \n",
    "    cv2.imwrite('messigray.png',img)\n",
    "    cv2.destroyALLWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted object no face mask: 81.28%\n",
      "[[337, 401, 101, 69]]\n",
      "predicted object no face mask: 83.14%\n",
      "[[340, 398, 96, 70]]\n",
      "predicted object no face mask: 88.42%\n",
      "[[337, 401, 103, 68]]\n",
      "predicted object no face mask: 83.78%\n",
      "[[339, 400, 99, 69]]\n",
      "predicted object no face mask: 80.19%\n",
      "[[340, 399, 96, 72]]\n",
      "predicted object no face mask: 83.38%\n",
      "[[339, 401, 99, 68]]\n",
      "predicted object no face mask: 80.59%\n",
      "[[340, 405, 97, 65]]\n",
      "predicted object no face mask: 96.14%\n",
      "[[293, 376, 127, 88]]\n",
      "predicted object no face mask: 97.12%\n",
      "[[292, 380, 129, 86]]\n",
      "predicted object no face mask: 96.71%\n",
      "[[295, 379, 125, 86]]\n",
      "predicted object no face mask: 78.18%\n",
      "[[347, 358, 99, 88]]\n",
      "predicted object no face mask: 92.65%\n",
      "[[284, 357, 110, 83]]\n",
      "predicted object no face mask: 97.57%\n",
      "[[245, 355, 138, 83]]\n",
      "predicted object no face mask: 97.33%\n",
      "[[261, 356, 124, 74]]\n",
      "predicted object no face mask: 96.02%\n",
      "[[252, 336, 108, 81]]\n",
      "predicted object no face mask: 97.32%\n",
      "[[263, 183, 112, 85]]\n",
      "predicted object no face mask: 89.19%\n",
      "[[256, 58, 136, 98]]\n",
      "predicted object no face mask: 95.26%\n",
      "[[241, 120, 136, 90]]\n",
      "predicted object no face mask: 97.20%\n",
      "[[269, 130, 132, 95]]\n",
      "predicted object no face mask: 90.66%\n",
      "[[282, 155, 115, 72]]\n",
      "predicted object no face mask: 96.01%\n",
      "[[285, 168, 95, 65]]\n",
      "predicted object no face mask: 97.20%\n",
      "[[285, 175, 90, 66]]\n",
      "predicted object no face mask: 96.12%\n",
      "[[285, 175, 93, 67]]\n",
      "predicted object no face mask: 97.36%\n",
      "[[288, 173, 92, 71]]\n",
      "predicted object no face mask: 96.78%\n",
      "predicted object no face mask: 82.85%\n",
      "[[240, 175, 95, 70], [396, 81, 54, 45]]\n",
      "predicted object no face mask: 84.67%\n",
      "predicted object no face mask: 79.84%\n",
      "[[145, 180, 96, 64], [382, 77, 63, 51]]\n",
      "predicted object no face mask: 94.96%\n",
      "predicted object no face mask: 92.88%\n",
      "[[69, 172, 112, 65], [373, 76, 64, 54]]\n",
      "predicted object no face mask: 93.82%\n",
      "predicted object no face mask: 58.85%\n",
      "[[55, 163, 115, 70], [399, 52, 71, 50]]\n",
      "predicted object no face mask: 97.82%\n",
      "[[118, 177, 81, 70]]\n",
      "predicted object no face mask: 92.44%\n",
      "[[174, 188, 69, 58]]\n",
      "predicted object no face mask: 92.54%\n",
      "[[212, 201, 60, 46]]\n",
      "predicted object no face mask: 90.41%\n",
      "[[234, 191, 65, 53]]\n",
      "predicted object no face mask: 81.37%\n",
      "[[214, 193, 77, 52]]\n",
      "predicted object no face mask: 93.61%\n",
      "[[201, 199, 71, 49]]\n",
      "predicted object no face mask: 90.51%\n",
      "[[215, 193, 76, 51]]\n",
      "predicted object no face mask: 84.07%\n",
      "[[227, 218, 80, 54]]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'predicted_class_label' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-04738a9760d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mscore_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mnms_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwinner_boxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_class_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobject_detection_analysis_with_nms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_colors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj_detections_in_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnms_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;31m#print(result)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-0527b78363a9>\u001b[0m in \u001b[0;36mobject_detection_analysis_with_nms\u001b[1;34m(test_img, class_labels, class_colors, obj_detections_in_layers, score_threshold, nms_threshold)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_class_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstart_x_pt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_y_pt\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox_color\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwinner_box_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_class_label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'predicted_class_label' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# importing the necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Creating a VideoCapture object to read the video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# Loop until the end of the video\n",
    "while (cap.isOpened()):\n",
    "\n",
    "\t# Capture frame-by-frame\n",
    "\tret, test_img = cap.read()\n",
    "\tscalefactor = 1.0/255.0\n",
    "\tnew_size = (416, 416)\n",
    "\tblob = cv2.dnn.blobFromImage(test_img, scalefactor, new_size, swapRB=True, crop=False)\n",
    "\tyolo_model.setInput(blob)\n",
    "\tobj_detections_in_layers = yolo_model.forward(output_layers)\n",
    "\tscore_threshold = 0.5\n",
    "\tnms_threshold = 0.4\n",
    "\tresult, winner_boxes, predicted_class_label = object_detection_analysis_with_nms(test_img, class_labels, class_colors, obj_detections_in_layers, score_threshold, nms_threshold)\n",
    "\t#print(result)\n",
    "\n",
    "\t# Display the resulting frame\n",
    "\t#cv2.imshow('Frame', frame)\n",
    "\tiou_image = test_img.copy()\n",
    " \n",
    "\t# print the ground truth and detection bounding boxes, and the IoU value\n",
    "\tprint(winner_boxes)\n",
    "\tiou_image = object_detection_iou(test_img, winner_boxes[0], predicted_class_label)\n",
    "\n",
    "\tcv2.imshow('demo',iou_image)\n",
    "\n",
    "\t# define q as the exit button\n",
    "\tif cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "\t\tbreak\n",
    "\n",
    "# release the video capture object\n",
    "cap.release()\n",
    "# Closes all the windows currently opened.\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
